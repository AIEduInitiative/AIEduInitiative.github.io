[
  {
    "objectID": "events/index.html",
    "href": "events/index.html",
    "title": "Events",
    "section": "",
    "text": "Meetup in Heidelberg: Unpacking the Black Box\n\n\n1 min\n\n\nProgress, changes, risks and challenges\n\n\n\nJul 8, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/tail/index.html",
    "href": "blog/tail/index.html",
    "title": "Painting Tails",
    "section": "",
    "text": "There are techniques for painting a region under a curve. But the experimental ggfx package offers an interesting alternative solution based on the blending modes familiar to users of Photoshop.\n\nlibrary(conflicted)\nlibrary(tidyverse)\nconflict_prefer_all(\"dplyr\", quiet = TRUE)\nconflict_prefer(\"as_date\", \"lubridate\")\nlibrary(scales)\nlibrary(ggfx)\nlibrary(patchwork)\nlibrary(wesanderson)\nlibrary(clock)\nlibrary(tidyquant)\n\n\ntheme_set(theme_bw())\n\n(cols &lt;- wes_palette(\"Royal1\"))\n\n\n\n\nThe advantage here is that the tail-painting aesthetic needs no information about the shape of the curve; only the limits on the x-axis.\nThe left plot shows the raw components without blending. The right plot is only retaining the red where there is a layer below.\n\np0 &lt;- tibble(outcome = rnorm(10000, 20, 2)) |&gt;\n  ggplot(aes(outcome)) +\n  scale_y_continuous(labels = label_percent())\n\np1 &lt;- p0 +\n  geom_density(adjust = 2, fill = cols[3]) +\n  annotate(\"rect\",\n    xmin = 15, xmax = 18, ymin = -Inf, ymax = Inf,\n    fill = cols[2]\n  ) + \n  labs(title = \"Without Blending\", y = \"Density\")\n\np2 &lt;- p0 +\n  as_reference(geom_density(adjust = 2, fill = cols[3]), id = \"density\") +\n  with_blend(annotate(\"rect\",\n    xmin = 15, xmax = 18, ymin = -Inf, ymax = Inf,\n    fill = cols[2]\n  ), bg_layer = \"density\", blend_type = \"atop\") + \n  labs(title = \"With Blending\", y = NULL)\n\np1 + p2\n\n\n\n\nOf course the red box could also be layered behind a density curve with alpha applied so it shows through. But if the preference is tail-only colouring, it’s a neat solution.\nBlending is actually a handy solution for any awkward shape. The same technique is used here with a time series ribbon summarising the median, lower and upper quartiles of a set of closing stock prices.\n\n\n\n\n\n\nNote\n\n\n\nTry this patch if having problems with tq_get\nThis chunk is using the development version of dplyr which introduces temporary grouping with .by.\n\n\n\ntickrs &lt;- c(\"AAPL\", \"NFLX\", \"TSLA\", \"ADBE\", \"META\", \"GOOG\", \"MSFT\")\n\np0 &lt;- tq_get(tickrs, get = \"stock.prices\", from = \"2022-01-01\") |&gt;\n  filter(!is.na(close)) |&gt; \n  reframe(\n    close = quantile(close, c(0.25, 0.5, 0.75)),\n    quantile = c(\"lower\", \"median\", \"upper\") |&gt; factor(),\n    .by = date\n  ) |&gt;\n  pivot_wider(names_from = quantile, values_from = close) |&gt;\n  ggplot(aes(date, median)) +\n  annotate(\"text\",\n    x = ymd(\"2022-03-16\"), y = 100,\n    label = \"Helpful\\nAnnotation\", colour = \"black\"\n  ) +\n  scale_y_continuous(limits = c(0, NA)) +\n  labs(x = NULL)\n\np1 &lt;- p0 +\n  geom_ribbon(aes(ymin = lower, ymax = upper), fill = cols[1]) +\n  geom_line(colour = cols[3]) +\n  annotate(\"rect\",\n    xmin = ymd(\"2022-03-01\"), xmax = ymd(\"2022-03-31\"),\n    ymin = -Inf, ymax = Inf, \n    fill = cols[4], colour = \"black\", linetype = \"dashed\"\n  ) + \n  labs(title = \"Without Blending\", y = \"Closing Price\")\n\np2 &lt;- p0 +\n  as_reference(geom_ribbon(aes(ymin = lower, ymax = upper), \n                           fill = cols[1]), id = \"ribbon\") +\n  with_blend(\n    annotate(\n      \"rect\",\n      xmin = ymd(\"2022-03-01\"), xmax = ymd(\"2022-03-31\"),\n      ymin = -Inf, ymax = Inf, \n      fill = cols[4], colour = \"black\", linetype = \"dashed\"\n      ),\n    bg_layer = \"ribbon\", blend_type = \"atop\"\n    ) +\n  geom_line(colour = cols[3]) + \n  labs(title = \"With Blending\", y = NULL)\n\np1 + p2 +\n  plot_annotation(title = \"Median Price Bounded by Upper & Lower Quartiles\")"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Painting Tails\n\n\n3 min\n\n\nIf you’re a cat, go find the nearest open pot of paint. But if you’re a data scientist, what to do?\n\n\n\nApr 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMoving House\n\n\n4 min\n\n\nLeaving Wordpress for a quieter life in Blogdown with the Hugo Academic theme\n\n\n\nJul 26, 2020\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "AIEdu",
    "section": "",
    "text": "Who We Are\n\n\n\nAI Educational Initiative \n\n\n \n\nImage by pch.vector on Freepik\n\n\nThe AI Educational Initiative was founded by a group of curious scientists with a deep interest in global AI breakthroughs. We are researchers from computer science, life sciences, or business. The founders are connected by the following shared vision, mission and goals:\nVision:\n\nFostering a responsible and inclusive AI landscape that benefits all of humanity\n\nMission:\n\nEducating ourselves and the society about the practical and positive applications of AI for the betterment of society and the environment\nAdvocating for appropriate and necessary regulations of AI in collaboration with other initiatives\nRaising awareness about the data protection and emphasizing the need to prioritize privacy protection\nUtilizing AI for scientific projects, such as environmental sustainability and biomedical initiatives\n\nGoals:\n\nOrganizing debates on AI and ethics to foster discussions and critical thinking on the responsible use of AI\nAmplifying the influence of AI, law, and ethics experts’ opinions in AI regulation debates\nEstablishing educational, awareness, and support programs to address concerns of the society such as losing jobs or facing lowered motivation due to AI advancements\nEncouraging authorities to create legislation and social programs that facilitate individuals in adapting to a world w ith AI, including learning new skills, finding new jobs, and maintaining motivation and drive"
  },
  {
    "objectID": "blog/plunge/index.html",
    "href": "blog/plunge/index.html",
    "title": "Moving House",
    "section": "",
    "text": "After reading up on Blogdown, I decided to take the plunge and leave Wordpress for a quieter life in Blogdown."
  },
  {
    "objectID": "blog/plunge/index.html#motivation",
    "href": "blog/plunge/index.html#motivation",
    "title": "Moving House",
    "section": "Motivation",
    "text": "Motivation\nMy former site looked pretty good. But it was expensive to maintain.\nI was spending more than I wished to get a performant site. I could have spent less, and perhaps I’m easily seduced by “bells & whistles”, e.g. CloudFlare Plus and “GoGeek” hosting. But a non-speedy site is a bit of a turn-off.\nAnd it wasn’t just cost. It also took a lot of non-R effort to publish a post with Rmarkdown in the way I wanted. My main interest is in writing R code. Not wrestling Wordpress and multiple plugins into submission.\nA reboot was also a chance to re-brand. When I originally set up thinkr.biz I was initially unaware of a similarly-named site in France. Although my personal blog posed no threat across the Channel, and we co-existed for a few years, I anyway prefer having something a little more unique."
  },
  {
    "objectID": "blog/plunge/index.html#why-blogdown",
    "href": "blog/plunge/index.html#why-blogdown",
    "title": "Moving House",
    "section": "Why Blogdown?",
    "text": "Why Blogdown?\nI like Yihui Xie’s Blogdown primarily because it simplifies the path from Rmarkdown to blog. No more WWE-style detour. I can tweak a line of code in Rmarkdown, serve_site, and immediately see the updated blog locally. When I’m ready to publish, I just drag the public folder into Netlify, and voilà it’s live."
  },
  {
    "objectID": "blog/plunge/index.html#my-personal-roadmap",
    "href": "blog/plunge/index.html#my-personal-roadmap",
    "title": "Moving House",
    "section": "My personal roadmap",
    "text": "My personal roadmap\nThere are different routes one can take. Here’s mine.\n\nChoose a theme\nIn his book, Yihui advises asking yourself: “Do I like this fancy theme so much that I will definitely not change it in the next couple of years?” It’s very sound advice. Nonetheless, I’m easily seduced, so explored all possible fancy themes. In part because I like creating my own graphic art, so I wanted something that could help these little creations shine.\nOddly, I started by looking at one of Yihui’s recommended themes and discarded it, only to return to it again much later after an exhaustive exploration of other themes. There are many superficially nice Hugo themes. But when you actually play with them, there’s little below the surface and/or an absence of serious upkeep.\nHugo Academic is not the most appealing in the “shop window”. But when you take it for a test spin, and really take it through its paces, it offers a richness, flexibility and investment that reeled me in. After customising it to my taste, and paring back optional bits I do not need, simply by switching them off, it gave me something I feel very happy with.\n\n\nMigrate\nThere are assisted migration paths, e.g. from Wordpress, discussed in the Blogdown book. However I wanted to review and upgrade the R code in my original posts (only a dozen or so at the time). So, one-by-one, I copied each Rmd file into the projects folder of my new site, tweaked the code, and used serve_site to see the end product.\nI took this approach because R, especially the tidyverse and its ecosystem, is rapidly evolving. For example, the latest release of dplyr has some great new column-wise and row-wise functions. And spread and gather have been superseded by the more capable pivot_wider and pivot_longer. So it was a chance to upgrade my code.\nFor one or two of the more processing-intensive projects, I used either cache = TRUE in the code chunk, or saveRDS and readRDS to load data prepared earlier.\n\n\nBuild\nAn option I haven’t yet pursued is to host all my website source files in a GIT repository. Then Netlify could call Hugo to render my website automatically. Right now, my site content is simple enough to be able to use the Build Website button in RStudio.\nThe web-site is a static build, so it’s fast out-of-the-box, i.e. no need for speed-inducing wallet-slimming plugins.\n\n\nDeploy\nNetlify is recommended by bookdown.org. And it’s free for personal projects. The only small annual cost is my domain name.\nChoosing a domain name, which one can do via Netflify, is a little tricky. Many of the ideas one might have, have already occurred to someone else. And when you do find something available, there’s always that niggling feeling there may be something better out there.\nI chose Quantum Jitter for several reasons:\n\nI often use ggplot2’s geom_jitter\nLike a Quant, I have an interest in using machine learning to assess stock fundamentals\nLike the quantum world, my work features statistics and randomness\nIt was available\n\nAfter running the build in RStudio, which for my site only takes a few minutes, I can simply drag my newly-created public folder into Netlify’s Deploys page and bingo, the site’s live in a jiffy.\nSo, if you are toying with the idea of moving house, I can recommend a quieter life in Blogdown."
  },
  {
    "objectID": "events/2023-07-08/index.html",
    "href": "events/2023-07-08/index.html",
    "title": "Meetup in Heidelberg: Unpacking the Black Box",
    "section": "",
    "text": "On 8th of July, we had the pleasure to host a meetup on Artificial Intelligence! Aimed at a general audience, we gathered AI scientists and users to share insights on their research, future plans, and concerns related to AI developments.\nStarted by engaging intro by the host Veli Vural Uslu, followed by a keynote lecture by Gürkan Solmaz, we learned about AI research in the context of making cities smarter and more livable.\nIn the following panel discussion, the audience engaged in a lively exchange with Gürkan, joined by Ana Victoria Ponce Bobadilla and Jude A., thrown into the file of questions by Magdalena Szczygieł-Buchner.\n\n\nThank you all who have joined us in Heidelberg last weekend! We hope you enjoyed it as much as we all did. We are looking forward to seeing you again - hopefully already this fall in Munich.\n\n\n\n\n\nWe are really grateful to Safak Chasan, Joel Ottosson, Denitsa Dragneva, Melania Barile, Zeynep Sena Ağım Uslu, and other LinkedIn-less volunteers who made this event possible!\nPS. Follow us for more info on AI and future events 😊"
  },
  {
    "objectID": "glossary/index.html",
    "href": "glossary/index.html",
    "title": "AI Glossary",
    "section": "",
    "text": "Image by Freepik\n\n\n\nThis glossary is initiated from the the website of Council of Europe. It contains the AI-related terms for general audience and AIEdu team will regularly update and maintain this list with new entries and also descriptive videos.\n\n\n\nA\n\n\nALGORITHM\nFinite suite of formal rules (logical operations, instructions) allowing to obtain a result from input elements. This suite can be the object of an automated execution process and rely on models designed through machine learning.\nARTIFICIAL INTELLIGENCE (AI)\nA set of sciences, theories and techniques whose purpose is to reproduce by a machine the cognitive abilities of a human being. Current developments aim to be able to entrust a machine with complex tasks previously delegated to a human.\nHowever, the term artificial intelligence is criticized by experts who distinguish between “strong” AI (who are able to contextualize very different specialized problems completely independently) and “weak” or “moderate” AI (who perform extremely well in their field of training). According to some experts, “strong” AI would require advances in basic research to be able to model the world as a whole and not just improvements in the performance of existing systems.\n\n\nB\n\n\nBIG DATA\nThe term “big data” refers to a large heterogeneous data set (open data, proprietary data, commercially purchased data).\nBias\nThere is a famous study where the students were asked to rate their driving skills amongst their peers. The results showed that 80 % asseded their skills to be in the top half of the group. This is of course impossible, and shows that the students have a bias in their assessment of their driver’s skills.\nBias meassures how far off a set of results are from the true value. In the case of the students, the true value is 50 %, making the bias 30 percentunits. In AI, bias pops up most notably when it comes to an AI model’s prediction. A model might perform well on a certain group of people and less well on an other. We expect, or aim, for the model to have the same performance for both groups, but in this case, the model has a bias for the first group of people and against the second group. Real worlds examples includes facial recognition models that are biases against people of color, and resume reasers, biases against people with foreign looking names.\n\n\nC\n\n\nCHATBOT (conversational agent)\nConversational agent that dialogues with its user (for example: empathic robots available to patients, or automated conversation services in customer relations).\n\n\nD\n\n\nDATABASE\nA database is a “container” storing data such as numbers, dates or words, which can be reprocessed by computer means to produce information; for example, numbers and names assembled and sorted to form a directory.\nDATA MINING (Data analysis and mining)\nDatamining makes it possible to analyze a large volume of data and bring out models, correlations and trends.\nDATA SCIENCE\nA broad grouping of mathematics, statistics, probability, computing, data visualization to extract knowledge from a heterogeneous set of data (images, sound, text, genomic data, social network links, physical measurements, etc.).\nThe methods and tools derived from artificial intelligence are part of this family.\nDEEP LEARNING\nSee Machine learning and Neural Network\n\n\nM\n\n\nMACHINE LEARNING\nMachine learning makes it possible to construct a mathematical model from data, including a large number of variables that are not known in advance. The parameters are configured as you go through a learning phase, which uses training data sets to find links and classifies them. The different machine learning methods are chosen by the designers according to the nature of the tasks to be performed (grouping, decision tree). These methods are usually classified into 3 categories: human-supervised learning, unsupervised learning, and unsupervised learning by reinforcement. These 3 categories group together different methods including neural networks, deep learning etc.\nExample of classification and applications:\n\nMETADATA\nData used to define, contextualize or characterize other data.\n\n\nN\n\n\nNEURAL NETWORK (artificial) / FORMAL NEURON\nAlgorithmic system, whose design was originally schematically inspired by the functioning of biological neurons and which, subsequently, came close to statistical methods.\nThe so-called formal neuron is designed as an automaton with a transfer function that transforms its inputs into outputs according to precise logical, arithmetic and symbolic rules. Assembled in a network, these formal neurons are able to quickly operate classifications and gradually learn to improve them.\nThis type of learning has been tested by tests on games (Go, video games). It is used for robotics, automated translation, etc.\n\n\nO\n\n\nOPEN DATA\nThe term refers to the public availability, by download, of structured databases. These data may be re-used in a non-monetary way under the conditions of a specific licence, which may in particular specify or prohibit certain purposes of re-use.\nOpen data is not to be confused with unitary public information available on Internet sites, the entire database of which cannot be downloaded (for example case law databases). It does not replace the mandatory publication of certain administrative or judicial measures or decisions already enacted by certain laws or regulations.\nFinally, confusion is sometimes created between the data (open data strictly speaking) and their means of processing (machine learning, data science) for different purposes (search engines, assistance in drafting acts, analysis of jurisprudential trends, anticipation of court decisions).\n\n\nP\n\n\nPERSONAL DATA\nInformation relating to an identified or identifiable natural person, directly or indirectly, by reference to one or more elements specific to that person.\nAmong these, sensitive data within the meaning of the General Data Protection Regulation concern personal data relating to racial or ethnic origin, political opinions, religious or philosophical beliefs, trade union membership, as well as genetic data, biometric data, data concerning health or concerning sex life or sexual orientation.\nPERSONAL DATA PROCESSING\nAny operation or set of operations performed or not using automated processes and applied to personal data or sets of data, such as collection, recording, organisation, structuring, storage, adaptation or modification, retrieval, consultation, use, communication by transmission, dissemination or any other form of making available, linking or interconnection, limitation, erasure or destruction.\nPROFILING\nAccording to Article 4(4) of the GDPR, personal data are processed for the purpose of evaluating certain aspects of a natural person’s life (economic situation, health, personal preferences, etc.).\nPSEUDONYMISATION\nAccording to Article 4 of the GDPR, personal data may no longer be attributed to a specific data subject without recourse to additional information, provided that this additional information is kept separately and subject to technical and organisational measures to ensure that the personal data are not attributed to an identified or identifiable natural person.\n\n\nV\n\n\nVariance If you ask people about the distance between the Paris and Amsterdam, you will most likely get an array of difference answeres. If you then go on and ask about the distance to the international space station, you can again expect to get an array of different guesses. Probably even more likely then the answeres to the first question. Since people have a better idea about the distance between cities, than the distance to space stations, the guesses to the first question will surely be less spread out than the guesses to the second. We say that the answeres to the second question have a higher variance, then the answeres to the first one; even though the the distance between Paris is slightly larger than the distance to the international space station.\nIn AI, variance often comes up when we speak about the spread of the AI model’s performance. It is ofcourse always desirable for a model to have high performance, however, in certain situation it might be wise to lower the overall performance if that means that we can lower the variance. For example, we prefere a self driving car that always drives modereatly well (low variance) to a car that sometimes drives excellent, and sometimes drives awefull."
  }
]